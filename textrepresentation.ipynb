{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":320111,"sourceType":"datasetVersion","datasetId":134715}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-02T07:34:12.369933Z","iopub.execute_input":"2025-06-02T07:34:12.370121Z","iopub.status.idle":"2025-06-02T07:34:14.573692Z","shell.execute_reply.started":"2025-06-02T07:34:12.370103Z","shell.execute_reply":"2025-06-02T07:34:14.572553Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nnltk.download('punkt')\nnltk.download('stopwords')\n\n# Load dataset\ndf = pd.read_csv('/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')\n\n# Basic cleaning function\ndef preprocess(text):\n    # Lowercase\n    text = text.lower()\n    # Remove HTML tags\n    text = re.sub(r'<.*?>', '', text)\n    # Remove punctuation\n    text = text.translate(str.maketrans('', '', string.punctuation))\n    # Tokenize\n    words = word_tokenize(text)\n    # Remove stopwords\n    stop_words = set(stopwords.words('english'))\n    words = [word for word in words if word not in stop_words]\n    return ' '.join(words)\n\n# Apply preprocessing to all reviews\ndf['clean_review'] = df['review'].apply(preprocess)\n\n# Display some results\ndf[['review', 'clean_review', 'sentiment']].head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T07:35:30.386474Z","iopub.execute_input":"2025-06-02T07:35:30.386878Z","iopub.status.idle":"2025-06-02T07:36:18.653446Z","shell.execute_reply.started":"2025-06-02T07:35:30.386852Z","shell.execute_reply":"2025-06-02T07:36:18.652329Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"                                              review  \\\n0  One of the other reviewers has mentioned that ...   \n1  A wonderful little production. <br /><br />The...   \n2  I thought this was a wonderful way to spend ti...   \n3  Basically there's a family where a little boy ...   \n4  Petter Mattei's \"Love in the Time of Money\" is...   \n\n                                        clean_review sentiment  \n0  one reviewers mentioned watching 1 oz episode ...  positive  \n1  wonderful little production filming technique ...  positive  \n2  thought wonderful way spend time hot summer we...  positive  \n3  basically theres family little boy jake thinks...  negative  \n4  petter matteis love time money visually stunni...  positive  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>clean_review</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>One of the other reviewers has mentioned that ...</td>\n      <td>one reviewers mentioned watching 1 oz episode ...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n      <td>wonderful little production filming technique ...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I thought this was a wonderful way to spend ti...</td>\n      <td>thought wonderful way spend time hot summer we...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Basically there's a family where a little boy ...</td>\n      <td>basically theres family little boy jake thinks...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n      <td>petter matteis love time money visually stunni...</td>\n      <td>positive</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"from collections import Counter\n\n# Combine all cleaned reviews into one big string\nall_text = ' '.join(df['clean_review'])\n\n# Split into individual words\nall_words = all_text.split()\n\n# Total number of words (including repetitions)\ntotal_words = len(all_words)\n\n# Total number of unique words (vocabulary size)\nunique_words = len(set(all_words))\n\n# Word frequency using Counter (optional)\nword_freq = Counter(all_words)\n\n# Display results\nprint(\"Total Words in Corpus:\", total_words)\nprint(\"Total Unique Words (Vocabulary):\", unique_words)\n\n# Show 10 most common words (optional)\nprint(\"\\nTop 10 Most Common Words:\")\nprint(word_freq.most_common(10))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T07:42:05.274137Z","iopub.execute_input":"2025-06-02T07:42:05.274541Z","iopub.status.idle":"2025-06-02T07:42:07.851527Z","shell.execute_reply.started":"2025-06-02T07:42:05.274510Z","shell.execute_reply":"2025-06-02T07:42:07.850349Z"}},"outputs":[{"name":"stdout","text":"Total Words in Corpus: 5992075\nTotal Unique Words (Vocabulary): 222320\n\nTop 10 Most Common Words:\n[('movie', 83508), ('film', 74468), ('one', 50369), ('like', 38825), ('good', 28483), ('even', 24280), ('would', 24001), ('time', 23266), ('really', 22894), ('see', 22432)]\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n# Use Keras Tokenizer to assign each word a unique integer\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(df['clean_review'])\n\n# Convert text to sequences of integers\nsequences = tokenizer.texts_to_sequences(df['clean_review'])\n\n# Vocabulary dictionary (word -> index)\nword_index = tokenizer.word_index\n\n# Optional: Pad sequences to the same length for neural networks\npadded_sequences = pad_sequences(sequences)\n\n# Show one example\nprint(\"Original review:\", df['clean_review'][0])\nprint(\"Integer encoded:\", sequences[0])\nprint(\"Padded sequence shape:\", padded_sequences.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T07:47:49.683512Z","iopub.execute_input":"2025-06-02T07:47:49.683885Z","iopub.status.idle":"2025-06-02T07:48:17.243493Z","shell.execute_reply.started":"2025-06-02T07:47:49.683859Z","shell.execute_reply":"2025-06-02T07:48:17.242681Z"}},"outputs":[{"name":"stderr","text":"2025-06-02 07:47:51.632805: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1748850471.874871      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1748850471.945696      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Original review: one reviewers mentioned watching 1 oz episode youll hooked right exactly happened methe first thing struck oz brutality unflinching scenes violence set right word go trust show faint hearted timid show pulls punches regards drugs sex violence hardcore classic use wordit called oz nickname given oswald maximum security state penitentary focuses mainly emerald city experimental section prison cells glass fronts face inwards privacy high agenda em city home manyaryans muslims gangstas latinos christians italians irish moreso scuffles death stares dodgy dealings shady agreements never far awayi would say main appeal show due fact goes shows wouldnt dare forget pretty pictures painted mainstream audiences forget charm forget romanceoz doesnt mess around first episode ever saw struck nasty surreal couldnt say ready watched developed taste oz got accustomed high levels graphic violence violence injustice crooked guards wholl sold nickel inmates wholl kill order get away well mannered middle class inmates turned prison bitches due lack street skills prison experience watching oz may become comfortable uncomfortable viewingthats get touch darker side\nInteger encoded: [3, 1809, 940, 56, 398, 3216, 286, 349, 3027, 107, 481, 470, 7413, 19, 57, 3098, 3216, 5386, 14836, 50, 472, 180, 107, 548, 52, 1605, 41, 8025, 5581, 11458, 41, 2359, 5835, 5493, 1339, 276, 472, 3260, 246, 233, 60879, 353, 3216, 11124, 240, 15441, 6674, 2419, 938, 60880, 2483, 1246, 24499, 422, 4558, 2376, 1079, 6877, 2830, 12562, 298, 60881, 16895, 213, 4902, 3570, 422, 236, 83188, 8154, 39826, 14837, 4990, 7606, 2319, 16896, 60882, 224, 8952, 7194, 12977, 8403, 33776, 34, 126, 21148, 7, 45, 166, 1172, 41, 550, 91, 159, 154, 436, 2845, 703, 85, 1144, 4167, 2353, 974, 703, 1282, 703, 83189, 58, 852, 88, 19, 286, 43, 103, 3098, 1451, 2067, 288, 45, 1422, 174, 1336, 1121, 3216, 83, 9932, 213, 1949, 1961, 472, 472, 7675, 6878, 4784, 13678, 2799, 31543, 6770, 13678, 380, 499, 14, 140, 13, 9590, 632, 693, 6770, 542, 1079, 19848, 550, 438, 808, 1860, 1079, 443, 56, 3216, 98, 302, 3603, 3116, 83190, 14, 1082, 3863, 392]\nPadded sequence shape: (50000, 1429)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from keras.utils import to_categorical\n\n# Example: Convert a sequence [3, 5, 1] to one-hot vectors (for a small vocab of size 6)\nexample_sequence = [3, 5, 1]\nvocab_size = len(word_index) + 1\none_hot_encoded = to_categorical(example_sequence, num_classes=vocab_size)\n\nprint(\"One-hot encoded vectors:\")\nprint(one_hot_encoded)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T08:03:47.088865Z","iopub.execute_input":"2025-06-02T08:03:47.089197Z","iopub.status.idle":"2025-06-02T08:03:47.097587Z","shell.execute_reply.started":"2025-06-02T08:03:47.089174Z","shell.execute_reply":"2025-06-02T08:03:47.096684Z"}},"outputs":[{"name":"stdout","text":"One-hot encoded vectors:\n[[0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 1. 0. ... 0. 0. 0.]]\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Assuming df is your DataFrame with original reviews in 'review' column\nimport re\n\ndef clean_text(text):\n    text = text.lower()\n    text = re.sub(r'<.*?>', '', text)   # Remove HTML tags\n    text = re.sub(r'[^a-z\\s]', '', text) # Remove punctuation and numbers\n    text = re.sub(r'\\s+', ' ', text).strip()\n    return text\n\nX_clean = df['review'].apply(clean_text)\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nvectorizer = CountVectorizer()\nX_bow = vectorizer.fit_transform(X_clean)\n\nvocab = vectorizer.vocabulary_\nword_counts = X_bow.sum(axis=0)\n\nword_freq = {word: word_counts[0, idx] for word, idx in vocab.items()}\nword_freq_sorted = sorted(word_freq.items(), key=lambda item: item[1], reverse=True)\n\nprint(\"Top 10 frequent words:\\n\")\nfor word, freq in word_freq_sorted[:10]:\n    print(f\"{word}: {freq}\")\n\nprint(f\"\\nVocabulary Size: {len(vocab)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T07:58:27.526050Z","iopub.execute_input":"2025-06-02T07:58:27.526341Z","iopub.status.idle":"2025-06-02T07:58:42.437431Z","shell.execute_reply.started":"2025-06-02T07:58:27.526320Z","shell.execute_reply":"2025-06-02T07:58:42.436662Z"}},"outputs":[{"name":"stdout","text":"Top 10 frequent words:\n\nthe: 650812\nand: 319428\nof: 288081\nto: 266297\nis: 210068\nin: 183153\nit: 151365\nthis: 145500\nthat: 135806\nwas: 95187\n\nVocabulary Size: 214594\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\n# Bi-gram vectorizer\nbigram_vectorizer = CountVectorizer(ngram_range=(2, 2))\nX_bigram = bigram_vectorizer.fit_transform(X_clean)\nbigram_vocab = bigram_vectorizer.vocabulary_\n\n# Tri-gram vectorizer\ntrigram_vectorizer = CountVectorizer(ngram_range=(3, 3))\nX_trigram = trigram_vectorizer.fit_transform(X_clean)\ntrigram_vocab = trigram_vectorizer.vocabulary_\n\nprint(f\"Vocabulary Size with Unigrams: {len(vocab)}\")\nprint(f\"Vocabulary Size with Bi-grams: {len(bigram_vocab)}\")\nprint(f\"Vocabulary Size with Tri-grams: {len(trigram_vocab)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T08:05:19.460737Z","iopub.execute_input":"2025-06-02T08:05:19.461419Z","iopub.status.idle":"2025-06-02T08:06:36.659620Z","shell.execute_reply.started":"2025-06-02T08:05:19.461384Z","shell.execute_reply":"2025-06-02T08:06:36.658496Z"}},"outputs":[{"name":"stdout","text":"Vocabulary Size with Unigrams: 214594\nVocabulary Size with Bi-grams: 2562875\nVocabulary Size with Tri-grams: 6653975\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\n# Initialize TF-IDF vectorizer\ntfidf_vectorizer = TfidfVectorizer()\n\n# Fit and transform cleaned text data\nX_tfidf = tfidf_vectorizer.fit_transform(X_clean)\n\n# Vocabulary: word -> index\ntfidf_vocab = tfidf_vectorizer.vocabulary_\n\n# IDF scores\nidf_scores = dict(zip(tfidf_vectorizer.get_feature_names_out(), tfidf_vectorizer.idf_))\n\n# Sort IDF scores (lowest to highest)\nsorted_idf = sorted(idf_scores.items(), key=lambda x: x[1])\n\nprint(f\"Vocabulary Size with TF-IDF: {len(tfidf_vocab)}\\n\")\n\nprint(\"Top 10 words with lowest IDF scores (most common words):\")\nfor word, score in sorted_idf[:10]:\n    print(f\"{word}: {score:.4f}\")\n\nprint(\"\\nTop 10 words with highest IDF scores (most unique words):\")\nfor word, score in sorted_idf[-10:]:\n    print(f\"{word}: {score:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T08:09:22.278037Z","iopub.execute_input":"2025-06-02T08:09:22.278364Z","iopub.status.idle":"2025-06-02T08:09:32.391084Z","shell.execute_reply.started":"2025-06-02T08:09:22.278341Z","shell.execute_reply":"2025-06-02T08:09:32.389756Z"}},"outputs":[{"name":"stdout","text":"Vocabulary Size with TF-IDF: 214594\n\nTop 10 words with lowest IDF scores (most common words):\nthe: 1.0093\nand: 1.0365\nof: 1.0532\nto: 1.0630\nthis: 1.1074\nis: 1.1124\nin: 1.1312\nit: 1.1707\nthat: 1.2289\nfor: 1.3458\n\nTop 10 words with highest IDF scores (most unique words):\nzyuranger: 11.1267\nzzzzip: 11.1267\nzzzzz: 11.1267\nzzzzzs: 11.1267\nzzzzzzzz: 11.1267\nzzzzzzzzz: 11.1267\nzzzzzzzzzzzz: 11.1267\nzzzzzzzzzzzzz: 11.1267\nzzzzzzzzzzzzzzzzzz: 11.1267\nzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz: 11.1267\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}